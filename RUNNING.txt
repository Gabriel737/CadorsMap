Steps to run the code on the local machine:
    - Setting up the environment: `python3 -m pip install -r requirements.txt`
    - `python main.py` should run the script to scrape the data
    - Put the data in a folder INPUT in the WeatherAugmenter folder and then run the WeathrAugmentation script.
    - ETL and ML scripts:
        - api/spark contains the jupyter notebooks for the ETL and the ML part
        - Since AWS Glue was getting very difficult to debug, we had to develop the script first on Google Collab and then modify it to suite AWS Glue
        - Structure of the scripts:
            - SparkETL.ipynb is the first script that runs. It does the pre-processing. Its a jupyter notebook so run the cells after mounting the drive and specifying the prefix
            - FinalDash.ipynb/SparkClassification.ipynb/SparkClustering.ipynb can be run in parallel. The output of these scripts are shown on the Quicksight dashboard.